{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Step 3: Training the LoRA Adapter\n",
        "\n",
        "**Goal**: Train Nemotron-3-Nano with LoRA on the MedMCQA dataset.\n",
        "\n",
        "In this notebook, we'll:\n",
        "1. Load the model with LoRA configuration from notebook 02\n",
        "2. Set up the SFTTrainer (Supervised Fine-Tuning Trainer) from TRL\n",
        "3. Configure training hyperparameters\n",
        "4. Train and monitor the loss\n",
        "5. Save the trained adapter\n",
        "6. Test the fine-tuned model\n",
        "\n",
        "**Stack used**: Transformers + PEFT + TRL (standard HuggingFace stack)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Setup and Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è HF_TOKEN not found in environment. Set it to avoid rate limits.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/nemotron-nano-v3-lora/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Authenticate with HuggingFace using token from environment\n",
        "if os.environ.get(\"HF_TOKEN\"):\n",
        "    login(token=os.environ[\"HF_TOKEN\"])\n",
        "    print(\"‚úÖ Logged in to HuggingFace Hub\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è HF_TOKEN not found in environment. Set it to avoid rate limits.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Load Training Configuration\n",
        "\n",
        "We'll use the configuration saved from notebook 02."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded configuration:\n",
            "{\n",
            "  \"model_name\": \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16\",\n",
            "  \"lora_config\": {\n",
            "    \"r\": 16,\n",
            "    \"lora_alpha\": 32,\n",
            "    \"lora_dropout\": 0.05,\n",
            "    \"target_modules\": [\n",
            "      \"o_proj\",\n",
            "      \"down_proj\",\n",
            "      \"v_proj\",\n",
            "      \"up_proj\",\n",
            "      \"q_proj\",\n",
            "      \"k_proj\"\n",
            "    ],\n",
            "    \"task_type\": \"TaskType.CAUSAL_LM\",\n",
            "    \"bias\": \"none\"\n",
            "  },\n",
            "  \"training_config\": {\n",
            "    \"max_seq_length\": 1024\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Load the config we saved in notebook 02\n",
        "config_path = Path(\"../outputs/training_config.json\")\n",
        "\n",
        "with open(config_path) as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "print(\"Loaded configuration:\")\n",
        "print(json.dumps(config, indent=2))\n",
        "\n",
        "MODEL_NAME = config[\"model_name\"]\n",
        "MAX_SEQ_LENGTH = config[\"training_config\"][\"max_seq_length\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Check GPU Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.10.0+cu128\n",
            "CUDA available: True\n",
            "CUDA version: 12.8\n",
            "GPU count: 8\n",
            "\n",
            "GPU 0: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "\n",
            "GPU 1: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "\n",
            "GPU 2: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "\n",
            "GPU 3: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "\n",
            "GPU 4: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "\n",
            "GPU 5: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "\n",
            "GPU 6: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "\n",
            "GPU 7: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "    \n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        total_memory_gb = props.total_memory / (1024**3)\n",
        "        print(f\"\\nGPU {i}: {props.name}\")\n",
        "        print(f\"  Total Memory: {total_memory_gb:.1f} GB\")\n",
        "else:\n",
        "    raise RuntimeError(\"No GPU available. Training requires a GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded:\n",
            "  Train: 182,822 examples\n",
            "  Validation: 4,183 examples\n",
            "\n",
            "Sample text (first 300 chars):\n",
            "<|im_start|>system\n",
            "You are a medical expert. Answer the multiple choice question by selecting the correct option and providing a brief explanation.<|im_end|>\n",
            "<|im_start|>user\n",
            "Question: Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parench...\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "# Load the formatted dataset from notebook 01\n",
        "formatted_dataset = load_from_disk(\"../data/medmcqa_formatted\")\n",
        "\n",
        "print(f\"Dataset loaded:\")\n",
        "print(f\"  Train: {len(formatted_dataset['train']):,} examples\")\n",
        "print(f\"  Validation: {len(formatted_dataset['validation']):,} examples\")\n",
        "\n",
        "print(f\"\\nSample text (first 300 chars):\")\n",
        "print(formatted_dataset[\"train\"][0][\"text\"][:300] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5 Load the Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer from: nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16\n",
            "‚úÖ Tokenizer loaded\n",
            "   Vocabulary size: 131,072\n",
            "   Pad token: '<|im_end|>'\n",
            "   Padding side: right\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(f\"Loading tokenizer from: {MODEL_NAME}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# Configure padding (required for batch training)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "tokenizer.padding_side = \"right\"  # Right padding for training (left for generation)\n",
        "\n",
        "print(f\"‚úÖ Tokenizer loaded\")\n",
        "print(f\"   Vocabulary size: {len(tokenizer):,}\")\n",
        "print(f\"   Pad token: {tokenizer.pad_token!r}\")\n",
        "print(f\"   Padding side: {tokenizer.padding_side}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading model: nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16\n",
            "This may take a few minutes...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6243/6243 [00:10<00:00, 585.11it/s, Materializing param=lm_head.weight]                                           \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Model loaded!\n",
            "   Model type: NemotronHForCausalLM\n",
            "   Model dtype: torch.bfloat16\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nLoading model: {MODEL_NAME}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "# Clear any cached memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Model loaded!\")\n",
        "print(f\"   Model type: {type(model).__name__}\")\n",
        "print(f\"   Model dtype: {model.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.6 Apply LoRA Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA Configuration:\n",
            "  Rank (r): 16\n",
            "  Alpha: 32\n",
            "  Scaling: 2.0\n",
            "  Dropout: 0.05\n",
            "  Target modules: {'v_proj', 'q_proj', 'up_proj', 'k_proj', 'o_proj', 'down_proj'}\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "\n",
        "# Recreate the LoRA config from our saved configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=config[\"lora_config\"][\"r\"],\n",
        "    lora_alpha=config[\"lora_config\"][\"lora_alpha\"],\n",
        "    lora_dropout=config[\"lora_config\"][\"lora_dropout\"],\n",
        "    target_modules=config[\"lora_config\"][\"target_modules\"],\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    bias=config[\"lora_config\"][\"bias\"],\n",
        ")\n",
        "\n",
        "print(\"LoRA Configuration:\")\n",
        "print(f\"  Rank (r): {lora_config.r}\")\n",
        "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"  Scaling: {lora_config.lora_alpha / lora_config.r}\")\n",
        "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
        "print(f\"  Target modules: {lora_config.target_modules}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Applying LoRA adapters...\n",
            "trainable params: 434,659,328 || all params: 32,012,596,672 || trainable%: 1.3578\n"
          ]
        }
      ],
      "source": [
        "# Apply LoRA to the model\n",
        "print(\"\\nApplying LoRA adapters...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "GPU Memory:\n",
            "  Allocated: 3.3 GB\n",
            "  Reserved: 3.3 GB\n"
          ]
        }
      ],
      "source": [
        "# Check memory after LoRA setup\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated() / (1024**3)\n",
        "    reserved = torch.cuda.memory_reserved() / (1024**3)\n",
        "    print(f\"\\nGPU Memory:\")\n",
        "    print(f\"  Allocated: {allocated:.1f} GB\")\n",
        "    print(f\"  Reserved: {reserved:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.7 Configure Training Arguments\n",
        "\n",
        "These hyperparameters control the training process. Think of them as knobs to tune.\n",
        "\n",
        "**Key parameters explained**:\n",
        "- `learning_rate`: Step size for weight updates. Too high = unstable, too low = slow.\n",
        "- `per_device_train_batch_size`: Examples per GPU per step. Limited by VRAM.\n",
        "- `gradient_accumulation_steps`: Simulate larger batches without more memory.\n",
        "- `num_train_epochs`: Full passes through the dataset.\n",
        "- `warmup_ratio`: Gradually increase LR at start (avoids early instability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
            "`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training configuration:\n",
            "  Epochs: 1\n",
            "  Per-device batch size: 2\n",
            "  Gradient accumulation: 8\n",
            "  Number of GPUs: 8\n",
            "  Effective batch size: 128\n",
            "  Learning rate: 0.0002\n",
            "  Warmup ratio: 0.03\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Output directory for checkpoints and logs\n",
        "OUTPUT_DIR = \"../outputs/lora_adapter\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # Output\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    \n",
        "    # Training duration\n",
        "    num_train_epochs=1,  # Start with 1 epoch for quick iteration\n",
        "    # max_steps=1000,  # Alternative: train for fixed number of steps\n",
        "    \n",
        "    # Batch size configuration\n",
        "    # Effective batch size = per_device_batch_size * gradient_accumulation_steps * num_gpus\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,  # Effective batch size = 2 * 8 = 16 per GPU\n",
        "    \n",
        "    # Learning rate schedule\n",
        "    learning_rate=2e-4,  # Higher than full fine-tuning since only LoRA weights update\n",
        "    warmup_ratio=0.03,   # 3% of training steps for warmup\n",
        "    lr_scheduler_type=\"cosine\",  # Gradually decrease LR\n",
        "    \n",
        "    # Optimization\n",
        "    optim=\"adamw_torch\",  # Standard AdamW optimizer\n",
        "    weight_decay=0.01,    # L2 regularization\n",
        "    max_grad_norm=1.0,    # Gradient clipping for stability\n",
        "    \n",
        "    # Precision\n",
        "    bf16=True,  # Use BF16 on A100/H100\n",
        "    \n",
        "    # Logging\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    logging_steps=10,\n",
        "    logging_first_step=True,\n",
        "    report_to=[\"tensorboard\"],  # Log to TensorBoard\n",
        "    \n",
        "    # Evaluation\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,  # Evaluate every 500 steps\n",
        "    \n",
        "    # Checkpointing\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=3,  # Keep only last 3 checkpoints to save space\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    \n",
        "    # Other settings\n",
        "    gradient_checkpointing=True,  # Trade compute for memory\n",
        "    dataloader_pin_memory=True,\n",
        "    remove_unused_columns=True,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# Calculate and display effective batch size\n",
        "effective_batch_size = (\n",
        "    training_args.per_device_train_batch_size\n",
        "    * training_args.gradient_accumulation_steps\n",
        "    * torch.cuda.device_count()\n",
        ")\n",
        "\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Per-device batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Number of GPUs: {torch.cuda.device_count()}\")\n",
        "print(f\"  Effective batch size: {effective_batch_size}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"  Warmup ratio: {training_args.warmup_ratio}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.8 Set Up the SFTTrainer\n",
        "\n",
        "The **SFTTrainer** (Supervised Fine-Tuning Trainer) from TRL handles:\n",
        "- Tokenizing the dataset on-the-fly\n",
        "- Packing multiple examples into sequences (optional, for efficiency)\n",
        "- Managing the training loop\n",
        "- Logging and checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
            "`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SFT Configuration:\n",
            "  Max sequence length: 1024\n",
            "  Dataset text field: text\n",
            "  Packing: False\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Create the SFT config (extends TrainingArguments)\n",
        "sft_config = SFTConfig(\n",
        "    # Inherit all training arguments\n",
        "    **training_args.to_dict(),\n",
        "    \n",
        "    # SFT-specific settings\n",
        "    # Note: TRL 0.27+ renamed max_seq_length -> max_length\n",
        "    max_length=MAX_SEQ_LENGTH,\n",
        "    dataset_text_field=\"text\",  # Column name in our dataset\n",
        "    packing=False,  # Don't pack multiple examples (cleaner loss signal)\n",
        ")\n",
        "\n",
        "print(f\"SFT Configuration:\")\n",
        "print(f\"  Max sequence length: {sft_config.max_length}\")\n",
        "print(f\"  Dataset text field: {sft_config.dataset_text_field}\")\n",
        "print(f\"  Packing: {sft_config.packing}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Trainer created!\n",
            "   Training examples: 182,822\n",
            "   Evaluation examples: 4,183\n"
          ]
        }
      ],
      "source": [
        "# Create the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=formatted_dataset[\"train\"],\n",
        "    eval_dataset=formatted_dataset[\"validation\"],\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Trainer created!\")\n",
        "print(f\"   Training examples: {len(trainer.train_dataset):,}\")\n",
        "print(f\"   Evaluation examples: {len(trainer.eval_dataset):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training plan:\n",
            "  Training examples: 182,822\n",
            "  Effective batch size: 128\n",
            "  Steps per epoch: 1,428\n",
            "  Total epochs: 1\n",
            "  Total steps: 1,428\n",
            "  Evaluation every: 500 steps\n",
            "  Checkpoints every: 500 steps\n"
          ]
        }
      ],
      "source": [
        "# Calculate total training steps\n",
        "num_training_examples = len(trainer.train_dataset)\n",
        "steps_per_epoch = num_training_examples // effective_batch_size\n",
        "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
        "\n",
        "print(f\"\\nTraining plan:\")\n",
        "print(f\"  Training examples: {num_training_examples:,}\")\n",
        "print(f\"  Effective batch size: {effective_batch_size}\")\n",
        "print(f\"  Steps per epoch: {steps_per_epoch:,}\")\n",
        "print(f\"  Total epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Total steps: {total_steps:,}\")\n",
        "print(f\"  Evaluation every: {training_args.eval_steps} steps\")\n",
        "print(f\"  Checkpoints every: {training_args.save_steps} steps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.9 Train the Model\n",
        "\n",
        "Now we train! Watch the loss metrics:\n",
        "- **train_loss**: Should decrease steadily\n",
        "- **eval_loss**: Should decrease but may plateau. If it increases while train_loss decreases, that's overfitting.\n",
        "\n",
        "**Pro tip**: You can monitor training in real-time with TensorBoard:\n",
        "```bash\n",
        "tensorboard --logdir=outputs/lora_adapter/logs\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting training...\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='389' max='11427' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  389/11427 8:25:22 < 240:14:09, 0.01 it/s, Epoch 0.03/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train!\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Training complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/nemotron-nano-v3-lora/.venv/lib/python3.10/site-packages/transformers/trainer.py:2174\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2172\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/nemotron-nano-v3-lora/.venv/lib/python3.10/site-packages/transformers/trainer.py:2536\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2529\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2530\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2532\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2533\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2534\u001b[0m )\n\u001b[1;32m   2535\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2536\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2539\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2541\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2542\u001b[0m ):\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/nemotron-nano-v3-lora/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:1264\u001b[0m, in \u001b[0;36mSFTTrainer.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_activation_offload_context:\n\u001b[0;32m-> 1264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/nemotron-nano-v3-lora/.venv/lib/python3.10/site-packages/transformers/trainer.py:3837\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3835\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3837\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
            "File \u001b[0;32m~/nemotron-nano-v3-lora/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:2852\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2851\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2852\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/nemotron-nano-v3-lora/.venv/lib/python3.10/site-packages/torch/_tensor.py:630\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    622\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    623\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    629\u001b[0m     )\n\u001b[0;32m--> 630\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/nemotron-nano-v3-lora/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:364\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    359\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/nemotron-nano-v3-lora/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:865\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    863\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"üöÄ Starting training...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Train!\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Summary:\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'train_result' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Print training summary\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Total steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtrain_result\u001b[49m\u001b[38;5;241m.\u001b[39mglobal_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_result\u001b[38;5;241m.\u001b[39mtraining_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(train_result, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_result' is not defined"
          ]
        }
      ],
      "source": [
        "# Print training summary\n",
        "print(\"\\nTraining Summary:\")\n",
        "print(f\"  Total steps: {train_result.global_step}\")\n",
        "print(f\"  Training loss: {train_result.training_loss:.4f}\")\n",
        "\n",
        "if hasattr(train_result, 'metrics'):\n",
        "    print(f\"\\nFinal metrics:\")\n",
        "    for key, value in train_result.metrics.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"  {key}: {value:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.10 Save the Trained Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the final model\n",
        "final_adapter_path = Path(OUTPUT_DIR) / \"final_adapter\"\n",
        "\n",
        "print(f\"Saving adapter to: {final_adapter_path}\")\n",
        "trainer.save_model(str(final_adapter_path))\n",
        "\n",
        "# Also save the tokenizer with the adapter\n",
        "tokenizer.save_pretrained(str(final_adapter_path))\n",
        "\n",
        "print(f\"\\n‚úÖ Model and tokenizer saved!\")\n",
        "\n",
        "# List saved files\n",
        "print(f\"\\nSaved files:\")\n",
        "for f in sorted(final_adapter_path.iterdir()):\n",
        "    size = f.stat().st_size / (1024**2)  # MB\n",
        "    print(f\"  {f.name}: {size:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.11 Test the Fine-Tuned Model\n",
        "\n",
        "Let's compare the fine-tuned model to the baseline from notebook 02.\n",
        "\n",
        "**Reminder**: The baseline (untrained LoRA) output was gibberish like:\n",
        "```\n",
        ", and,,,,,,,, in, and,, in,,,, in and,, in,,, and and in in,...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare the same test prompt from notebook 02\n",
        "test_prompt = \"\"\"<|im_start|>system\n",
        "You are a medical expert. Answer the multiple choice question by selecting the correct option and providing a brief explanation.<|im_end|>\n",
        "<|im_start|>user\n",
        "Question: Which vitamin is essential for blood clotting?\n",
        "\n",
        "A) Vitamin A\n",
        "B) Vitamin C\n",
        "C) Vitamin K\n",
        "D) Vitamin D<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "print(f\"Test prompt length: {inputs['input_ids'].shape[1]} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate response\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "# Decode and print\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "print(\"Generated response (FINE-TUNED MODEL):\")\n",
        "print(\"=\"*60)\n",
        "print(generated_text)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with another question\n",
        "test_prompt_2 = \"\"\"<|im_start|>system\n",
        "You are a medical expert. Answer the multiple choice question by selecting the correct option and providing a brief explanation.<|im_end|>\n",
        "<|im_start|>user\n",
        "Question: What is the most common cause of peptic ulcer disease?\n",
        "\n",
        "A) Stress\n",
        "B) Spicy food\n",
        "C) Helicobacter pylori infection\n",
        "D) Alcohol consumption<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "inputs_2 = tokenizer(test_prompt_2, return_tensors=\"pt\")\n",
        "inputs_2 = {k: v.to(model.device) for k, v in inputs_2.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs_2 = model.generate(\n",
        "        **inputs_2,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "generated_text_2 = tokenizer.decode(outputs_2[0], skip_special_tokens=False)\n",
        "\n",
        "print(\"Generated response (Question 2):\")\n",
        "print(\"=\"*60)\n",
        "print(generated_text_2)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.12 Final Evaluation on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run final evaluation\n",
        "print(\"Running final evaluation on validation set...\")\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Final Evaluation Results:\")\n",
        "print(\"=\"*60)\n",
        "for key, value in eval_results.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "In this notebook, we:\n",
        "\n",
        "1. **Loaded** the model with LoRA configuration\n",
        "2. **Configured** training hyperparameters (LR, batch size, etc.)\n",
        "3. **Set up** the SFTTrainer from TRL\n",
        "4. **Trained** the LoRA adapter on MedMCQA\n",
        "5. **Saved** the trained adapter (~2GB vs 60GB for full model)\n",
        "6. **Tested** the fine-tuned model on sample questions\n",
        "\n",
        "### Key Observations\n",
        "\n",
        "| Metric | Before Training | After Training |\n",
        "|--------|-----------------|----------------|\n",
        "| Output quality | Gibberish | Coherent medical answers |\n",
        "| Loss | ~12 | ~1-2 (depends on training) |\n",
        "| Trainable params | 435M (1.36%) | Same |\n",
        "\n",
        "### Saved Artifacts\n",
        "\n",
        "- `outputs/lora_adapter/final_adapter/`: The trained LoRA weights\n",
        "- `outputs/lora_adapter/logs/`: TensorBoard training logs\n",
        "- `outputs/lora_adapter/checkpoint-*/`: Intermediate checkpoints\n",
        "\n",
        "## ‚è≠Ô∏è Next Steps\n",
        "\n",
        "In the next notebook (`04_unsloth_comparison.ipynb`), we'll:\n",
        "- Compare training with Unsloth optimization\n",
        "- Measure speed and memory improvements\n",
        "- Discuss when to use each approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"‚úÖ GPU memory cleared. Ready for next notebook!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
