{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîß Step 2: Loading the Model & Configuring LoRA\n",
        "\n",
        "**Goal**: Load Nemotron-3-Nano and set up LoRA adapters using the standard HuggingFace stack.\n",
        "\n",
        "In this notebook, we'll:\n",
        "1. Load the base model and tokenizer\n",
        "2. Understand the model architecture\n",
        "3. Configure LoRA parameters\n",
        "4. Verify the setup before training\n",
        "\n",
        "**Stack used**: Transformers + PEFT (no Unsloth yet ‚Äî that comes in notebook 04)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Install Dependencies\n",
        "\n",
        "First, let's make sure we have the required packages installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è HF_TOKEN not found in environment. Set it to avoid rate limits.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/nemotron-nano-v3-lora/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Dependencies are managed in pyproject.toml\n",
        "# Run `uv sync` in the project root to install all required packages\n",
        "\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Authenticate with HuggingFace using token from environment\n",
        "if os.environ.get(\"HF_TOKEN\"):\n",
        "    login(token=os.environ[\"HF_TOKEN\"])\n",
        "    print(\"‚úÖ Logged in to HuggingFace Hub\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è HF_TOKEN not found in environment. Set it to avoid rate limits.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Check GPU Availability\n",
        "\n",
        "Before loading a 30B model, let's verify we have sufficient GPU resources.\n",
        "\n",
        "**Requirements**:\n",
        "- A100 80GB (recommended) or H100\n",
        "- CUDA available with BF16 support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.10.0+cu128\n",
            "CUDA available: True\n",
            "CUDA version: 12.8\n",
            "GPU count: 8\n",
            "\n",
            "GPU 0: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "  Compute Capability: 8.0\n",
            "  BF16 Supported: True\n",
            "\n",
            "GPU 1: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "  Compute Capability: 8.0\n",
            "  BF16 Supported: True\n",
            "\n",
            "GPU 2: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "  Compute Capability: 8.0\n",
            "  BF16 Supported: True\n",
            "\n",
            "GPU 3: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "  Compute Capability: 8.0\n",
            "  BF16 Supported: True\n",
            "\n",
            "GPU 4: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "  Compute Capability: 8.0\n",
            "  BF16 Supported: True\n",
            "\n",
            "GPU 5: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "  Compute Capability: 8.0\n",
            "  BF16 Supported: True\n",
            "\n",
            "GPU 6: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "  Compute Capability: 8.0\n",
            "  BF16 Supported: True\n",
            "\n",
            "GPU 7: NVIDIA A100-SXM4-80GB\n",
            "  Total Memory: 79.3 GB\n",
            "  Compute Capability: 8.0\n",
            "  BF16 Supported: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check CUDA availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "    \n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        total_memory_gb = props.total_memory / (1024**3)\n",
        "        print(f\"\\nGPU {i}: {props.name}\")\n",
        "        print(f\"  Total Memory: {total_memory_gb:.1f} GB\")\n",
        "        print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
        "        print(f\"  BF16 Supported: {props.major >= 8}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU available. This notebook requires a GPU with at least 40GB VRAM.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Load the Tokenizer\n",
        "\n",
        "The tokenizer converts text to token IDs (and back). Think of it as a serialization format.\n",
        "\n",
        "Key concepts:\n",
        "- **Vocabulary**: The set of all tokens the model knows (~128k for Nemotron)\n",
        "- **Special tokens**: Control tokens like `<|im_start|>`, `<|im_end|>`, `<pad>`, etc.\n",
        "- **Token IDs**: Integer indices into the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer type: TokenizersBackend\n",
            "Vocabulary size: 131,072\n",
            "Model max length: 262,144\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "print(f\"Tokenizer type: {type(tokenizer).__name__}\")\n",
        "print(f\"Vocabulary size: {len(tokenizer):,}\")\n",
        "print(f\"Model max length: {tokenizer.model_max_length:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Special tokens:\n",
            "  BOS (beginning of sequence): '<s>' -> 1\n",
            "  EOS (end of sequence): '<|im_end|>' -> 11\n",
            "  PAD (padding): None -> None\n",
            "  UNK (unknown): '<unk>' -> 0\n"
          ]
        }
      ],
      "source": [
        "# Check special tokens\n",
        "print(\"Special tokens:\")\n",
        "print(f\"  BOS (beginning of sequence): {tokenizer.bos_token!r} -> {tokenizer.bos_token_id}\")\n",
        "print(f\"  EOS (end of sequence): {tokenizer.eos_token!r} -> {tokenizer.eos_token_id}\")\n",
        "print(f\"  PAD (padding): {tokenizer.pad_token!r} -> {tokenizer.pad_token_id}\")\n",
        "print(f\"  UNK (unknown): {tokenizer.unk_token!r} -> {tokenizer.unk_token_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set pad_token to: '<|im_end|>'\n",
            "Padding side: left\n"
          ]
        }
      ],
      "source": [
        "# Set padding token if not set (required for batch training)\n",
        "# Common practice: use EOS token as padding token for decoder-only models\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    print(f\"Set pad_token to: {tokenizer.pad_token!r}\")\n",
        "\n",
        "# Left padding is preferred for generation (keeps the rightmost tokens intact)\n",
        "tokenizer.padding_side = \"left\"\n",
        "print(f\"Padding side: {tokenizer.padding_side}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Test Tokenization\n",
        "\n",
        "Let's see how our formatted medical QA text gets tokenized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample text (first 500 chars):\n",
            "<|im_start|>system\n",
            "You are a medical expert. Answer the multiple choice question by selecting the correct option and providing a brief explanation.<|im_end|>\n",
            "<|im_start|>user\n",
            "Question: Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchyma\n",
            "\n",
            "A) Hyperplasia\n",
            "B) Hyperophy\n",
            "C) Atrophy\n",
            "D) Dyplasia<|im_end|>\n",
            "<|im_start|>assistant\n",
            "The correct answer is C) Atrophy.\n",
            "\n",
            "Explanation: Chronic urethral obstruction because of urinary calculi, prostat\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "# Load our formatted dataset to test\n",
        "from datasets import load_from_disk\n",
        "\n",
        "formatted_dataset = load_from_disk(\"../data/medmcqa_formatted\")\n",
        "sample_text = formatted_dataset[\"train\"][0][\"text\"]\n",
        "\n",
        "print(\"Sample text (first 500 chars):\")\n",
        "print(sample_text[:500])\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tokenization result:\n",
            "  Input IDs shape: torch.Size([1, 173])\n",
            "  Attention mask shape: torch.Size([1, 173])\n",
            "  Number of tokens: 173\n",
            "\n",
            "First 20 token IDs: [10, 25708, 1010, 4568, 1584, 1261, 9371, 16967, 1046, 3450, 1278, 6245, 10284, 4098, 1536, 31277, 1278, 6298, 7091, 1321]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the sample\n",
        "tokens = tokenizer(sample_text, return_tensors=\"pt\")\n",
        "\n",
        "print(f\"\\nTokenization result:\")\n",
        "print(f\"  Input IDs shape: {tokens['input_ids'].shape}\")\n",
        "print(f\"  Attention mask shape: {tokens['attention_mask'].shape}\")\n",
        "print(f\"  Number of tokens: {tokens['input_ids'].shape[1]}\")\n",
        "print(f\"\\nFirst 20 token IDs: {tokens['input_ids'][0][:20].tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded first 50 tokens:\n",
            "<|im_start|>system\n",
            "You are a medical expert. Answer the multiple choice question by selecting the correct option and providing a brief explanation.<|im_end|>\n",
            "<|im_start|>user\n",
            "Question: Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney\n"
          ]
        }
      ],
      "source": [
        "# Decode back to text to verify\n",
        "decoded = tokenizer.decode(tokens['input_ids'][0][:50])\n",
        "print(f\"Decoded first 50 tokens:\")\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatML token analysis:\n",
            "  '<|im_start|>' -> [10]\n",
            "  '<|im_end|>' -> [11]\n",
            "  'system' -> [25708]\n",
            "  'user' -> [3263]\n",
            "  'assistant' -> [1503, 19464]\n"
          ]
        }
      ],
      "source": [
        "# Check how ChatML special tokens are tokenized\n",
        "special_tokens = [\"<|im_start|>\", \"<|im_end|>\", \"system\", \"user\", \"assistant\"]\n",
        "\n",
        "print(\"ChatML token analysis:\")\n",
        "for token in special_tokens:\n",
        "    ids = tokenizer.encode(token, add_special_tokens=False)\n",
        "    print(f\"  '{token}' -> {ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Analyze Token Lengths\n",
        "\n",
        "We need to choose an appropriate `max_seq_length` for training. Too short = truncation, too long = wasted memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:02<00:00, 1715.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Token length statistics (n=5000):\n",
            "  Min: 69\n",
            "  Max: 2,111\n",
            "  Mean: 206\n",
            "  Median: 171\n",
            "  95th percentile: 441\n",
            "  99th percentile: 751\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Sample a subset for analysis (full dataset takes too long)\n",
        "sample_size = min(5000, len(formatted_dataset[\"train\"]))\n",
        "sample_indices = np.random.choice(len(formatted_dataset[\"train\"]), sample_size, replace=False)\n",
        "\n",
        "token_lengths = []\n",
        "for idx in tqdm(sample_indices, desc=\"Tokenizing samples\"):\n",
        "    text = formatted_dataset[\"train\"][int(idx)][\"text\"]\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
        "    token_lengths.append(tokens['input_ids'].shape[1])\n",
        "\n",
        "print(f\"\\nToken length statistics (n={sample_size}):\")\n",
        "print(f\"  Min: {min(token_lengths):,}\")\n",
        "print(f\"  Max: {max(token_lengths):,}\")\n",
        "print(f\"  Mean: {np.mean(token_lengths):,.0f}\")\n",
        "print(f\"  Median: {np.median(token_lengths):,.0f}\")\n",
        "print(f\"  95th percentile: {np.percentile(token_lengths, 95):,.0f}\")\n",
        "print(f\"  99th percentile: {np.percentile(token_lengths, 99):,.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Recommendation:\n",
            "   99th percentile: 751 tokens\n",
            "   Recommended max_seq_length: 1024\n",
            "\n",
            "   This will truncate ~1% of examples but saves significant memory.\n",
            "   For training, we'll use max_seq_length = 1024 (covers ~95% without truncation)\n"
          ]
        }
      ],
      "source": [
        "# Recommend max_seq_length based on analysis\n",
        "p99 = np.percentile(token_lengths, 99)\n",
        "\n",
        "# Round up to nearest power of 2 for efficiency\n",
        "recommended_max_len = 2 ** int(np.ceil(np.log2(p99)))\n",
        "\n",
        "print(f\"\\nüìä Recommendation:\")\n",
        "print(f\"   99th percentile: {p99:.0f} tokens\")\n",
        "print(f\"   Recommended max_seq_length: {recommended_max_len}\")\n",
        "print(f\"\")\n",
        "print(f\"   This will truncate ~1% of examples but saves significant memory.\")\n",
        "print(f\"   For training, we'll use max_seq_length = 1024 (covers ~95% without truncation)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 Load the Base Model\n",
        "\n",
        "Now let's load Nemotron-3-Nano. This is a 30B parameter model, but only ~3B parameters activate per token (MoE architecture).\n",
        "\n",
        "**Key settings**:\n",
        "- `torch_dtype=torch.bfloat16`: Use BF16 precision (halves memory vs FP32)\n",
        "- `device_map=\"auto\"`: Automatically distribute across available GPUs\n",
        "- `attn_implementation=\"flash_attention_2\"`: Use FlashAttention-2 for memory efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16\n",
            "This may take a few minutes...\n",
            "\n",
            "Available GPU memory: 79.3 GB\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "# Check available memory before loading\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)\n",
        "    print(f\"\\nAvailable GPU memory: {free_memory / (1024**3):.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6243/6243 [00:10<00:00, 590.23it/s, Materializing param=lm_head.weight]                                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Load the model with optimal settings\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    # Use FlashAttention-2 if available (significant memory savings)\n",
        "    # attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else None,\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory allocated: 3.2 GB\n",
            "GPU memory reserved: 3.2 GB\n"
          ]
        }
      ],
      "source": [
        "# Check memory usage after loading\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
        "    reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
        "    print(f\"GPU memory allocated: {allocated:.1f} GB\")\n",
        "    print(f\"GPU memory reserved: {reserved:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.7 Explore the Model Architecture\n",
        "\n",
        "Understanding the architecture helps us know:\n",
        "1. Which layers can be targeted with LoRA\n",
        "2. How the MoE (Mixture of Experts) routing works\n",
        "3. The hybrid Mamba-Transformer structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model type: NemotronHForCausalLM\n",
            "Model dtype: torch.bfloat16\n",
            "\n",
            "Model configuration:\n",
            "  Hidden size: 2688\n",
            "  Num layers: 52\n",
            "  Num attention heads: 32\n",
            "  Vocabulary size: 131072\n"
          ]
        }
      ],
      "source": [
        "# Print model summary\n",
        "print(f\"Model type: {type(model).__name__}\")\n",
        "print(f\"Model dtype: {model.dtype}\")\n",
        "print(f\"\\nModel configuration:\")\n",
        "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
        "print(f\"  Num layers: {model.config.num_hidden_layers}\")\n",
        "print(f\"  Num attention heads: {model.config.num_attention_heads}\")\n",
        "print(f\"  Vocabulary size: {model.config.vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Parameter count:\n",
            "  Total: 31.58B\n",
            "  Trainable: 31.58B\n",
            "  Trainable %: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Count parameters\n",
        "def count_parameters(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total, trainable\n",
        "\n",
        "total_params, trainable_params = count_parameters(model)\n",
        "print(f\"\\nParameter count:\")\n",
        "print(f\"  Total: {total_params / 1e9:.2f}B\")\n",
        "print(f\"  Trainable: {trainable_params / 1e9:.2f}B\")\n",
        "print(f\"  Trainable %: {100 * trainable_params / total_params:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Found 9 unique linear layer types:\n",
            "  - down_proj\n",
            "  - in_proj\n",
            "  - k_proj\n",
            "  - lm_head\n",
            "  - o_proj\n",
            "  - out_proj\n",
            "  - q_proj\n",
            "  - up_proj\n",
            "  - v_proj\n"
          ]
        }
      ],
      "source": [
        "# Find all linear layer names (potential LoRA targets)\n",
        "def find_linear_layers(model):\n",
        "    \"\"\"Find all linear layer names in the model.\"\"\"\n",
        "    linear_layers = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            # Get the layer name (last part)\n",
        "            layer_name = name.split('.')[-1]\n",
        "            linear_layers.add(layer_name)\n",
        "    return sorted(linear_layers)\n",
        "\n",
        "linear_layers = find_linear_layers(model)\n",
        "print(f\"\\nFound {len(linear_layers)} unique linear layer types:\")\n",
        "for layer in linear_layers:\n",
        "    print(f\"  - {layer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.8 Configure LoRA\n",
        "\n",
        "Now we'll set up LoRA adapters. These are small trainable matrices injected into specific layers.\n",
        "\n",
        "**Key parameters** (see AGENTS.md for detailed explanation):\n",
        "- `r` (rank): Controls capacity. Higher = more parameters, better fit. Start with 16-32.\n",
        "- `lora_alpha`: Scaling factor. Rule of thumb: 2x the rank.\n",
        "- `target_modules`: Which layers get LoRA adapters.\n",
        "- `lora_dropout`: Regularization to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA Configuration:\n",
            "  Rank (r): 16\n",
            "  Alpha: 32\n",
            "  Scaling factor: 2.0\n",
            "  Dropout: 0.05\n",
            "  Target modules: {'o_proj', 'down_proj', 'v_proj', 'up_proj', 'q_proj', 'k_proj'}\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    # Core LoRA parameters\n",
        "    r=16,                      # Rank: capacity vs memory trade-off\n",
        "    lora_alpha=32,             # Scaling factor (2x rank is a good default)\n",
        "    lora_dropout=0.05,         # Regularization\n",
        "    \n",
        "    # Which layers to target\n",
        "    # For Nemotron: attention projections + MLP layers\n",
        "    # Note: This model doesn't have gate_proj (no SwiGLU gating)\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
        "        \"up_proj\", \"down_proj\",                   # MLP\n",
        "    ],\n",
        "    \n",
        "    # Task type for causal language modeling\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    \n",
        "    # Other settings\n",
        "    bias=\"none\",               # Don't train biases (reduces params)\n",
        "    inference_mode=False,      # We're training, not inferencing\n",
        ")\n",
        "\n",
        "print(\"LoRA Configuration:\")\n",
        "print(f\"  Rank (r): {lora_config.r}\")\n",
        "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"  Scaling factor: {lora_config.lora_alpha / lora_config.r}\")\n",
        "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
        "print(f\"  Target modules: {lora_config.target_modules}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying LoRA adapters to model...\n",
            "‚úÖ LoRA adapters added!\n"
          ]
        }
      ],
      "source": [
        "# Apply LoRA to the model\n",
        "print(\"Applying LoRA adapters to model...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"‚úÖ LoRA adapters added!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 434,659,328 || all params: 32,012,596,672 || trainable%: 1.3578\n"
          ]
        }
      ],
      "source": [
        "# Check the new parameter counts\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Detailed parameter breakdown:\n",
            "  Total parameters: 32.01B\n",
            "  Trainable (LoRA): 434.66M\n",
            "  Frozen (base): 31.58B\n",
            "  Trainable %: 1.3578%\n",
            "\n",
            "Memory savings:\n",
            "  Without LoRA, we'd need to store gradients for 32.0B params\n",
            "  With LoRA, we only store gradients for 434.7M params\n",
            "  That's 74x fewer gradient values!\n"
          ]
        }
      ],
      "source": [
        "# Detailed breakdown\n",
        "total_params, trainable_params = count_parameters(model)\n",
        "\n",
        "print(f\"\\nDetailed parameter breakdown:\")\n",
        "print(f\"  Total parameters: {total_params / 1e9:.2f}B\")\n",
        "print(f\"  Trainable (LoRA): {trainable_params / 1e6:.2f}M\")\n",
        "print(f\"  Frozen (base): {(total_params - trainable_params) / 1e9:.2f}B\")\n",
        "print(f\"  Trainable %: {100 * trainable_params / total_params:.4f}%\")\n",
        "print(f\"\")\n",
        "print(f\"Memory savings:\")\n",
        "print(f\"  Without LoRA, we'd need to store gradients for {total_params/1e9:.1f}B params\")\n",
        "print(f\"  With LoRA, we only store gradients for {trainable_params/1e6:.1f}M params\")\n",
        "print(f\"  That's {total_params / trainable_params:.0f}x fewer gradient values!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.9 Verify the Setup with a Forward Pass\n",
        "\n",
        "Let's make sure everything works by running a forward pass on a sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([1, 173])\n",
            "Input device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Prepare a sample input\n",
        "sample_text = formatted_dataset[\"train\"][0][\"text\"]\n",
        "inputs = tokenizer(sample_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "# Move to GPU\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
        "print(f\"Input device: {inputs['input_ids'].device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NemotronH requires an initialized `NemotronHHybridDynamicCache` to return a cache. None was provided, so no cache will be returned.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Forward pass successful!\n",
            "  Output logits shape: torch.Size([1, 173, 131072])\n",
            "  Loss: 12.0128\n"
          ]
        }
      ],
      "source": [
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "\n",
        "print(f\"\\n‚úÖ Forward pass successful!\")\n",
        "print(f\"  Output logits shape: {outputs.logits.shape}\")\n",
        "print(f\"  Loss: {outputs.loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.10 Test Text Generation (Optional)\n",
        "\n",
        "Let's see what the base model (with untrained LoRA) produces.\n",
        "This gives us a baseline to compare against after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt length: 66 tokens\n"
          ]
        }
      ],
      "source": [
        "# Create a test prompt\n",
        "test_prompt = \"\"\"<|im_start|>system\n",
        "You are a medical expert. Answer the multiple choice question by selecting the correct option and providing a brief explanation.<|im_end|>\n",
        "<|im_start|>user\n",
        "Question: Which vitamin is essential for blood clotting?\n",
        "\n",
        "A) Vitamin A\n",
        "B) Vitamin C\n",
        "C) Vitamin K\n",
        "D) Vitamin D<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "print(f\"Prompt length: {inputs['input_ids'].shape[1]} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated response (BASE MODEL + UNTRAINED LoRA):\n",
            "============================================================\n",
            "<|im_start|>system\n",
            "You are a medical expert. Answer the multiple choice question by selecting the correct option and providing a brief explanation.<|im_end|>\n",
            "<|im_start|>user\n",
            "Question: Which vitamin is essential for blood clotting?\n",
            "\n",
            "A) Vitamin A\n",
            "B) Vitamin C\n",
            "C) Vitamin K\n",
            "D) Vitamin D<|im_end|>\n",
            "<|im_start|>assistant\n",
            ", and,,,,,,,, in, and,, in,,,, in and,, in,,, and and in in,,, and and, in in,,,, in and, and and and and, in,, and and, and and and and over in and, and, and, and, and,, in, and, in,, and and and in and, and in and in and in,, and, and,, and in and in and in,,,, in,, in, and, and, in in and and, and in,,, and, and,,,,,,,, in, in, in in,, and\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Generate response\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "# Decode and print\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "print(\"Generated response (BASE MODEL + UNTRAINED LoRA):\")\n",
        "print(\"=\"*60)\n",
        "print(generated_text)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.11 Save the Configuration (Optional)\n",
        "\n",
        "We can save the LoRA configuration for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuration saved to ../outputs/training_config.json\n",
            "\n",
            "Config contents:\n",
            "{\n",
            "  \"model_name\": \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16\",\n",
            "  \"lora_config\": {\n",
            "    \"r\": 16,\n",
            "    \"lora_alpha\": 32,\n",
            "    \"lora_dropout\": 0.05,\n",
            "    \"target_modules\": [\n",
            "      \"o_proj\",\n",
            "      \"down_proj\",\n",
            "      \"v_proj\",\n",
            "      \"up_proj\",\n",
            "      \"q_proj\",\n",
            "      \"k_proj\"\n",
            "    ],\n",
            "    \"task_type\": \"TaskType.CAUSAL_LM\",\n",
            "    \"bias\": \"none\"\n",
            "  },\n",
            "  \"training_config\": {\n",
            "    \"max_seq_length\": 1024\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Create outputs directory if it doesn't exist\n",
        "outputs_dir = Path(\"../outputs\")\n",
        "outputs_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save config as JSON for reference\n",
        "config_dict = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"lora_config\": {\n",
        "        \"r\": lora_config.r,\n",
        "        \"lora_alpha\": lora_config.lora_alpha,\n",
        "        \"lora_dropout\": lora_config.lora_dropout,\n",
        "        \"target_modules\": list(lora_config.target_modules),  # Convert set to list for JSON\n",
        "        \"task_type\": str(lora_config.task_type),\n",
        "        \"bias\": lora_config.bias,\n",
        "    },\n",
        "    \"training_config\": {\n",
        "        \"max_seq_length\": 1024,  # Our recommendation from analysis\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(outputs_dir / \"training_config.json\", \"w\") as f:\n",
        "    json.dump(config_dict, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Configuration saved to {outputs_dir / 'training_config.json'}\")\n",
        "print(\"\\nConfig contents:\")\n",
        "print(json.dumps(config_dict, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "In this notebook, we:\n",
        "\n",
        "1. **Verified GPU resources** ‚Äî Checked CUDA availability and memory\n",
        "2. **Loaded the tokenizer** ‚Äî Configured special tokens and padding\n",
        "3. **Analyzed token lengths** ‚Äî Determined optimal `max_seq_length` (1024)\n",
        "4. **Loaded the base model** ‚Äî Nemotron-3-Nano with BF16 precision\n",
        "5. **Explored architecture** ‚Äî Found target layers for LoRA\n",
        "6. **Configured LoRA** ‚Äî Set up adapters with rank=16, alpha=32\n",
        "7. **Verified the setup** ‚Äî Successful forward pass with ~0.1% trainable params\n",
        "8. **Tested generation** ‚Äî Baseline output from untrained model\n",
        "\n",
        "### Key Numbers to Remember\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Total parameters | ~30B |\n",
        "| Trainable (LoRA) | ~30-50M |\n",
        "| Trainable % | ~0.1% |\n",
        "| Recommended max_seq_length | 1024 |\n",
        "\n",
        "## ‚è≠Ô∏è Next Step\n",
        "\n",
        "In the next notebook (`03_training.ipynb`), we'll:\n",
        "- Set up the SFTTrainer from TRL\n",
        "- Configure training hyperparameters\n",
        "- Train the model on our formatted MedMCQA dataset\n",
        "- Monitor loss and save checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPU memory cleared. Ready for training notebook!\n"
          ]
        }
      ],
      "source": [
        "# Cleanup (free GPU memory for next notebook)\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "print(\"‚úÖ GPU memory cleared. Ready for training notebook!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
